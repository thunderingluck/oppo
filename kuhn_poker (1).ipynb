{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH7K9kxX5Ju1"
      },
      "source": [
        "Info set indexing helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iMycQpUr5Gki"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# --------- infoset indexing ---------\n",
        "\n",
        "# For player 1:\n",
        "#  idx 0..2 : (card=0,1,2, history=\"\")\n",
        "#  idx 3..5 : (card=0,1,2, history=\"cb\")\n",
        "def p1_infoset_index(card, history):\n",
        "    if history == \"\":\n",
        "        return card\n",
        "    elif history == \"cb\":\n",
        "        return 3 + card\n",
        "    else:\n",
        "        raise ValueError(f\"invalid p1 history {history}\")\n",
        "\n",
        "# For player 2:\n",
        "#  idx 0..2 : (card=0,1,2, history=\"c\")\n",
        "#  idx 3..5 : (card=0,1,2, history=\"b\")\n",
        "def p2_infoset_index(card, history):\n",
        "    if history == \"c\":\n",
        "        return card\n",
        "    elif history == \"b\":\n",
        "        return 3 + card\n",
        "    else:\n",
        "        raise ValueError(f\"invalid p2 history {history}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcRbpRZd5PHI"
      },
      "source": [
        "2. Tabular policies\n",
        "\n",
        "Each player: a vector of 6 logits → Bernoulli over “aggressive” action:\n",
        "\n",
        "first decision: 0 = check, 1 = bet\n",
        "\n",
        "second-layer decision: 0 = fold, 1 = call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BHRcdxRC5SLH"
      },
      "outputs": [],
      "source": [
        "class KuhnPolicy(nn.Module):\n",
        "    def __init__(self, num_infosets=6):\n",
        "        super().__init__()\n",
        "        # one scalar logit per infoset\n",
        "        self.logits = nn.Parameter(torch.zeros(num_infosets))\n",
        "\n",
        "    def action_dist(self, infoset_indices):\n",
        "        \"\"\"\n",
        "        infoset_indices: LongTensor [B] of infoset ids (0..5)\n",
        "        returns Bernoulli distribution over action 1 (bet/call)\n",
        "        \"\"\"\n",
        "        logits = self.logits[infoset_indices]  # [B]\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return torch.distributions.Bernoulli(probs=probs)\n",
        "\n",
        "    def action_and_logp(self, infoset_index):\n",
        "        idx = torch.tensor([infoset_index], dtype=torch.long)\n",
        "        dist = self.action_dist(idx)\n",
        "        a = dist.sample()            # 0 or 1\n",
        "        logp = dist.log_prob(a)      # [1]\n",
        "        return int(a.item()), logp[0]\n",
        "\n",
        "    def probs_for_all_infosets(self):\n",
        "        with torch.no_grad():\n",
        "            return torch.sigmoid(self.logits).cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvXSRtOI5Yxn"
      },
      "source": [
        "One episode of Kuhn Poker self-play\n",
        "\n",
        "We simulate until terminal, collecting each player’s decisions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UMNQ9y3b5ZPY"
      },
      "outputs": [],
      "source": [
        "def kuhn_deal():\n",
        "    cards = [0, 1, 2]\n",
        "    random.shuffle(cards)\n",
        "    return cards[0], cards[1]  # (p1_card, p2_card)\n",
        "\n",
        "def kuhn_episode(pi1: KuhnPolicy, pi2: KuhnPolicy):\n",
        "    \"\"\"\n",
        "    Run one Kuhn Poker episode with self-play policies.\n",
        "    Returns:\n",
        "      p1_infos, p1_actions, p1_logps, p2_infos, p2_actions, p2_logps, r1, r2\n",
        "    \"\"\"\n",
        "    p1_card, p2_card = kuhn_deal()\n",
        "    history = \"\"\n",
        "\n",
        "    # storage for decisions\n",
        "    p1_infos, p1_actions, p1_logps = [], [], []\n",
        "    p2_infos, p2_actions, p2_logps = [], [], []\n",
        "\n",
        "    # P1 acts first\n",
        "    # ---- P1 first decision ----\n",
        "    i1 = p1_infoset_index(p1_card, history)\n",
        "    a1, logp1 = pi1.action_and_logp(i1)   # 0=check, 1=bet\n",
        "    p1_infos.append(i1)\n",
        "    p1_actions.append(a1)\n",
        "    p1_logps.append(logp1)\n",
        "\n",
        "    if a1 == 0:  # check\n",
        "        history = \"c\"\n",
        "        # ---- P2 decision after check ----\n",
        "        i2 = p2_infoset_index(p2_card, history)\n",
        "        a2, logp2 = pi2.action_and_logp(i2)  # 0=check, 1=bet\n",
        "        p2_infos.append(i2)\n",
        "        p2_actions.append(a2)\n",
        "        p2_logps.append(logp2)\n",
        "\n",
        "        if a2 == 0:  # cc -> showdown, pot 2\n",
        "            history = \"cc\"\n",
        "            pot = 2\n",
        "            if p1_card > p2_card:\n",
        "                r1 = pot / 2\n",
        "            else:\n",
        "                r1 = -pot / 2\n",
        "            r2 = -r1\n",
        "\n",
        "        else:       # cb -> P1 acts (call/fold)\n",
        "            history = \"cb\"\n",
        "            i1_2 = p1_infoset_index(p1_card, history)\n",
        "            a1_2, logp1_2 = pi1.action_and_logp(i1_2)  # 0=fold, 1=call\n",
        "            p1_infos.append(i1_2)\n",
        "            p1_actions.append(a1_2)\n",
        "            p1_logps.append(logp1_2)\n",
        "\n",
        "            if a1_2 == 0:  # cbf -> P1 folds, loses 1\n",
        "                history = \"cbf\"\n",
        "                r1 = -1.0\n",
        "                r2 = 1.0\n",
        "            else:          # cbc -> showdown, pot 4\n",
        "                history = \"cbc\"\n",
        "                pot = 4\n",
        "                if p1_card > p2_card:\n",
        "                    r1 = pot / 2\n",
        "                else:\n",
        "                    r1 = -pot / 2\n",
        "                r2 = -r1\n",
        "\n",
        "    else:  # a1==1 bet\n",
        "        history = \"b\"\n",
        "        # ---- P2 decision after bet ----\n",
        "        i2 = p2_infoset_index(p2_card, history)\n",
        "        a2, logp2 = pi2.action_and_logp(i2)  # 0=fold, 1=call\n",
        "        p2_infos.append(i2)\n",
        "        p2_actions.append(a2)\n",
        "        p2_logps.append(logp2)\n",
        "\n",
        "        if a2 == 0:  # bf -> P2 folds, P1 wins 1\n",
        "            history = \"bf\"\n",
        "            r1 = 1.0\n",
        "            r2 = -1.0\n",
        "        else:        # bc -> showdown, pot 4\n",
        "            history = \"bc\"\n",
        "            pot = 4\n",
        "            if p1_card > p2_card:\n",
        "                r1 = pot / 2\n",
        "            else:\n",
        "                r1 = -pot / 2\n",
        "            r2 = -r1\n",
        "\n",
        "    # convert lists -> tensors\n",
        "    p1_infos = torch.tensor(p1_infos, dtype=torch.long)\n",
        "    p1_actions = torch.tensor(p1_actions, dtype=torch.float32)\n",
        "    p1_logps = torch.stack(p1_logps) if len(p1_logps) > 0 else torch.tensor([])\n",
        "\n",
        "    p2_infos = torch.tensor(p2_infos, dtype=torch.long)\n",
        "    p2_actions = torch.tensor(p2_actions, dtype=torch.float32)\n",
        "    p2_logps = torch.stack(p2_logps) if len(p2_logps) > 0 else torch.tensor([])\n",
        "\n",
        "    return p1_infos, p1_actions, p1_logps, p2_infos, p2_actions, p2_logps, r1, r2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTJeadj05ak_"
      },
      "source": [
        "Batch rollout for PPO\n",
        "\n",
        "We collect many episodes into one batch:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EiNA5VTq5dEQ"
      },
      "outputs": [],
      "source": [
        "def kuhn_rollout_batch(pi1, pi2, batch_size):\n",
        "    \"\"\"\n",
        "    Collect `batch_size` episodes.\n",
        "    Returns dict with trajectories for both players.\n",
        "    \"\"\"\n",
        "\n",
        "    p1_infos, p1_actions, p1_logps = [], [], []\n",
        "    p2_infos, p2_actions, p2_logps = [], [], []\n",
        "\n",
        "    # per-decision returns (one scalar per decision taken)\n",
        "    p1_returns_per_decision, p2_returns_per_decision = [], []\n",
        "\n",
        "    # per-episode returns (for logging only)\n",
        "    p1_rewards, p2_rewards = [], []\n",
        "\n",
        "    for _ in range(batch_size):\n",
        "        (i1, a1, lp1,\n",
        "         i2, a2, lp2,\n",
        "         r1, r2) = kuhn_episode(pi1, pi2)\n",
        "\n",
        "        # --- player 1 ---\n",
        "        if i1.numel() > 0:\n",
        "            p1_infos.append(i1)\n",
        "            p1_actions.append(a1)\n",
        "            p1_logps.append(lp1)\n",
        "            # repeat the episode return for each decision in this episode\n",
        "            p1_returns_per_decision.append(\n",
        "                torch.full_like(lp1, float(r1))\n",
        "            )\n",
        "\n",
        "        # --- player 2 ---\n",
        "        if i2.numel() > 0:\n",
        "            p2_infos.append(i2)\n",
        "            p2_actions.append(a2)\n",
        "            p2_logps.append(lp2)\n",
        "            p2_returns_per_decision.append(\n",
        "                torch.full_like(lp2, float(r2))\n",
        "            )\n",
        "\n",
        "        p1_rewards.append(r1)\n",
        "        p2_rewards.append(r2)\n",
        "\n",
        "    def cat_list(tensors):\n",
        "        if len(tensors) == 0:\n",
        "            return torch.tensor([], dtype=torch.float32)\n",
        "        return torch.cat(tensors, dim=0)\n",
        "\n",
        "    return {\n",
        "        \"p1_infos\": cat_list(p1_infos).long(),\n",
        "        \"p1_actions\": cat_list(p1_actions).float(),\n",
        "        \"p1_logps\": cat_list(p1_logps).float(),\n",
        "        \"p1_returns_per_decision\": cat_list(p1_returns_per_decision).float(),\n",
        "        \"p1_rewards\": torch.tensor(p1_rewards, dtype=torch.float32),\n",
        "\n",
        "        \"p2_infos\": cat_list(p2_infos).long(),\n",
        "        \"p2_actions\": cat_list(p2_actions).float(),\n",
        "        \"p2_logps\": cat_list(p2_logps).float(),\n",
        "        \"p2_returns_per_decision\": cat_list(p2_returns_per_decision).float(),\n",
        "        \"p2_rewards\": torch.tensor(p2_rewards, dtype=torch.float32),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y_8cX2I5l8r"
      },
      "source": [
        "PPO Loss for Tabular Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "74Skhuuu5sCk"
      },
      "outputs": [],
      "source": [
        "def kuhn_ppo_loss(\n",
        "    policy,\n",
        "    infos,\n",
        "    actions,\n",
        "    logp_old,\n",
        "    returns_per_decision,\n",
        "    clip_eps=0.2,\n",
        "    entropy_coef=0.01,\n",
        "):\n",
        "    \"\"\"\n",
        "    infos: [N] infoset indices\n",
        "    actions: [N] (0/1)\n",
        "    logp_old: [N]\n",
        "    returns_per_decision: [N] Monte Carlo returns for each decision\n",
        "    \"\"\"\n",
        "    if infos.numel() == 0:\n",
        "        return torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "    # advantages: centered + normalized per batch\n",
        "    adv = returns_per_decision\n",
        "    adv = adv - adv.mean()\n",
        "    adv = adv / (adv.std() + 1e-8)\n",
        "\n",
        "    dist = policy.action_dist(infos)\n",
        "    logp = dist.log_prob(actions)\n",
        "\n",
        "    ratio = torch.exp(logp - logp_old)\n",
        "    surr1 = ratio * adv\n",
        "    surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * adv\n",
        "    pg_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "    entropy = dist.entropy().mean()\n",
        "    loss = pg_loss - entropy_coef * entropy\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhInLSSP5wrj"
      },
      "source": [
        "Self-play training with modes (PPO / O-PPO-Last / EMA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yoj9GKO35xV8"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_kuhn(\n",
        "    mode=\"ppo\",          # \"ppo\", \"oppo_last\", \"oppo_ema\"\n",
        "    num_iters=400,\n",
        "    batch_size=512,\n",
        "    lr=3e-4,\n",
        "    omega=0.3,\n",
        "    beta=0.6,\n",
        "):\n",
        "    pi1 = KuhnPolicy().to(device)\n",
        "    pi2 = KuhnPolicy().to(device)\n",
        "\n",
        "    opt1 = optim.SGD(pi1.parameters(), lr=lr)\n",
        "    opt2 = optim.SGD(pi2.parameters(), lr=lr)\n",
        "\n",
        "    prev_grads1 = [torch.zeros_like(p) for p in pi1.parameters()]\n",
        "    prev_grads2 = [torch.zeros_like(p) for p in pi2.parameters()]\n",
        "    mom1 = [torch.zeros_like(p) for p in pi1.parameters()]\n",
        "    mom2 = [torch.zeros_like(p) for p in pi2.parameters()]\n",
        "\n",
        "    p1_return_hist, p2_return_hist = [], []\n",
        "    exploit_list = []\n",
        "\n",
        "    for it in range(num_iters):\n",
        "        traj = kuhn_rollout_batch(pi1, pi2, batch_size)\n",
        "\n",
        "        # ------ Player 1 update ------\n",
        "        opt1.zero_grad()\n",
        "        loss1 = kuhn_ppo_loss(\n",
        "            pi1,\n",
        "            traj[\"p1_infos\"],\n",
        "            traj[\"p1_actions\"],\n",
        "            traj[\"p1_logps\"],\n",
        "            traj[\"p1_returns_per_decision\"],\n",
        "        )\n",
        "        loss1.backward()\n",
        "\n",
        "        if mode == \"ppo\":\n",
        "            opt1.step()\n",
        "\n",
        "        elif mode == \"oppo_last\":\n",
        "            with torch.no_grad():\n",
        "                cur_grads = [p.grad.detach().clone() if p.grad is not None\n",
        "                             else torch.zeros_like(p)\n",
        "                             for p in pi1.parameters()]\n",
        "            for p, g_t, g_prev in zip(pi1.parameters(), cur_grads, prev_grads1):\n",
        "                if p.grad is not None:\n",
        "                    p.grad = (1.0 + omega) * g_t - omega * g_prev\n",
        "            opt1.step()\n",
        "            prev_grads1 = cur_grads\n",
        "\n",
        "        elif mode == \"oppo_ema\":\n",
        "            with torch.no_grad():\n",
        "                cur_grads = [p.grad.detach().clone() if p.grad is not None\n",
        "                             else torch.zeros_like(p)\n",
        "                             for p in pi1.parameters()]\n",
        "            new_mom = []\n",
        "            for p, g_t, m_prev in zip(pi1.parameters(), cur_grads, mom1):\n",
        "                m_t = (1.0 - beta) * g_t + beta * m_prev\n",
        "                new_mom.append(m_t)\n",
        "                if p.grad is not None:\n",
        "                    p.grad = (1.0 + omega) * g_t - omega * m_t\n",
        "            opt1.step()\n",
        "            mom1 = new_mom\n",
        "\n",
        "        # ------ Player 2 update (symmetric) ------\n",
        "        opt2.zero_grad()\n",
        "        loss2 = kuhn_ppo_loss(\n",
        "            pi2,\n",
        "            traj[\"p2_infos\"],\n",
        "            traj[\"p2_actions\"],\n",
        "            traj[\"p2_logps\"],\n",
        "            traj[\"p2_returns_per_decision\"],\n",
        "        )\n",
        "        loss2.backward()\n",
        "\n",
        "        if mode == \"ppo\":\n",
        "            opt2.step()\n",
        "\n",
        "        elif mode == \"oppo_last\":\n",
        "            with torch.no_grad():\n",
        "                cur_grads = [p.grad.detach().clone() if p.grad is not None\n",
        "                             else torch.zeros_like(p)\n",
        "                             for p in pi2.parameters()]\n",
        "            for p, g_t, g_prev in zip(pi2.parameters(), cur_grads, prev_grads2):\n",
        "                if p.grad is not None:\n",
        "                    p.grad = (1.0 + omega) * g_t - omega * g_prev\n",
        "            opt2.step()\n",
        "            prev_grads2 = cur_grads\n",
        "\n",
        "        elif mode == \"oppo_ema\":\n",
        "            with torch.no_grad():\n",
        "                cur_grads = [p.grad.detach().clone() if p.grad is not None\n",
        "                             else torch.zeros_like(p)\n",
        "                             for p in pi2.parameters()]\n",
        "            new_mom = []\n",
        "            for p, g_t, m_prev in zip(pi2.parameters(), cur_grads, mom2):\n",
        "                m_t = (1.0 - beta) * g_t + beta * m_prev\n",
        "                new_mom.append(m_t)\n",
        "                if p.grad is not None:\n",
        "                    p.grad = (1.0 + omega) * g_t - omega * m_t\n",
        "            opt2.step()\n",
        "            mom2 = new_mom\n",
        "\n",
        "        # logging averages\n",
        "        p1_return_hist.append(traj[\"p1_rewards\"].mean().item())\n",
        "        p2_return_hist.append(traj[\"p2_rewards\"].mean().item())\n",
        "\n",
        "        # exact exploitability\n",
        "        exp, br1, br2, v = kuhn_exploitability(pi1, pi2)\n",
        "        exploit_list.append(exp)\n",
        "\n",
        "        if (it + 1) % 50 == 0:\n",
        "            print(f\"iter {it+1} | mode={mode} | \"\n",
        "                  f\"E[R1]={p1_return_hist[-1]:.3f} | exploit={exp:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"p1_returns\": p1_return_hist,\n",
        "        \"p2_returns\": p2_return_hist,\n",
        "        \"exploitability\": exploit_list,\n",
        "        \"pi1\": pi1,\n",
        "        \"pi2\": pi2,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO58-97-50QG"
      },
      "source": [
        "Running and plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qumdvqFz6kVt"
      },
      "source": [
        "**Exploitability**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0yIA5k_06olM"
      },
      "outputs": [],
      "source": [
        "def enumerate_pure_strategies(num_infosets=6):\n",
        "    \"\"\"\n",
        "    Returns list of pure strategies.\n",
        "    Each strategy is a list of length 6 of 0/1 actions.\n",
        "    \"\"\"\n",
        "    strategies = []\n",
        "    for i in range(2**num_infosets):\n",
        "        s = [(i >> k) & 1 for k in range(num_infosets)]\n",
        "        strategies.append(s)\n",
        "    return strategies\n",
        "\n",
        "PURE_STRATS = enumerate_pure_strategies(6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gu2hYuBO6v_0"
      },
      "outputs": [],
      "source": [
        "def kuhn_exploitability(pi1, pi2):\n",
        "    \"\"\"\n",
        "    Exact exploitability of (pi1, pi2) in Kuhn Poker.\n",
        "    \"\"\"\n",
        "    v = kuhn_expected_value(pi1, pi2)\n",
        "    br1 = best_response_value_to_p2(pi2)\n",
        "    br2 = best_response_value_to_p1(pi1)\n",
        "\n",
        "    exploit = 0.5 * ((br1 - v) + (v - br2))  # symmetric definition\n",
        "    return exploit, br1, br2, v\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rMH4hxxj6smE"
      },
      "outputs": [],
      "source": [
        "def kuhn_payoff_given_actions(card1, card2, history):\n",
        "    \"\"\"Computes terminal payoff to player1 given history + showdown.\"\"\"\n",
        "    if history == \"cc\":\n",
        "        pot = 2\n",
        "        return pot/2 if card1 > card2 else -pot/2\n",
        "    elif history == \"cbc\":\n",
        "        pot = 4\n",
        "        return pot/2 if card1 > card2 else -pot/2\n",
        "    elif history == \"bc\":\n",
        "        pot = 4\n",
        "        return pot/2 if card1 > card2 else -pot/2\n",
        "    elif history == \"bf\":   # P2 folded\n",
        "        return 1\n",
        "    elif history == \"cbf\":  # P1 folded\n",
        "        return -1\n",
        "    else:\n",
        "        raise ValueError(\"invalid terminal history\", history)\n",
        "\n",
        "\n",
        "# def kuhn_expected_value(pi1, pi2, pure1=None, pure2=None):\n",
        "#     \"\"\"\n",
        "#     pure1/pure2 are lists of 6 deterministic actions (0/1) or None.\n",
        "#     If None, use the stochastic policies.\n",
        "#     Returns expected value to player 1 under all 6 card deals.\n",
        "#     \"\"\"\n",
        "\n",
        "#     total = 0.0\n",
        "#     for card1 in [0,1,2]:\n",
        "#         for card2 in [0,1,2]:\n",
        "#             if card1 == card2:\n",
        "#                 continue  # impossible deal\n",
        "#             p = 1/6  # uniform over 6 permutations\n",
        "\n",
        "#             # ----- P1 first decision -----\n",
        "#             i1 = p1_infoset_index(card1, \"\")\n",
        "#             if pure1:\n",
        "#                 a1 = pure1[i1]\n",
        "#                 logp_dummy = None\n",
        "#             else:\n",
        "#                 a1, _ = pi1.action_and_logp(i1)\n",
        "\n",
        "#             if a1 == 0:\n",
        "#                 # P1 check\n",
        "#                 # ----- P2 decision -----\n",
        "#                 i2 = p2_infoset_index(card2, \"c\")\n",
        "#                 if pure2:\n",
        "#                     a2 = pure2[i2]\n",
        "#                 else:\n",
        "#                     a2, _ = pi2.action_and_logp(i2)\n",
        "\n",
        "#                 if a2 == 0:\n",
        "#                     payoff = kuhn_payoff_given_actions(card1, card2, \"cc\")\n",
        "#                 else:\n",
        "#                     # P2 bets, P1 responds\n",
        "#                     i1b = p1_infoset_index(card1, \"cb\")\n",
        "#                     if pure1:\n",
        "#                         a1b = pure1[i1b]\n",
        "#                     else:\n",
        "#                         a1b, _ = pi1.action_and_logp(i1b)\n",
        "\n",
        "#                     if a1b == 0:\n",
        "#                         payoff = kuhn_payoff_given_actions(card1, card2, \"cbf\")\n",
        "#                     else:\n",
        "#                         payoff = kuhn_payoff_given_actions(card1, card2, \"cbc\")\n",
        "\n",
        "#             else:\n",
        "#                 # P1 bet\n",
        "#                 i2 = p2_infoset_index(card2, \"b\")\n",
        "#                 if pure2:\n",
        "#                     a2 = pure2[i2]\n",
        "#                 else:\n",
        "#                     a2, _ = pi2.action_and_logp(i2)\n",
        "\n",
        "#                 if a2 == 0:\n",
        "#                     payoff = kuhn_payoff_given_actions(card1, card2, \"bf\")\n",
        "#                 else:\n",
        "#                     payoff = kuhn_payoff_given_actions(card1, card2, \"bc\")\n",
        "\n",
        "#             total += p * payoff\n",
        "\n",
        "#     return total\n",
        "def kuhn_prob_action1_p1(pi1, card, history):\n",
        "    # probability that P1 takes action 1 (bet/call) at (card, history)\n",
        "    idx = p1_infoset_index(card, history)\n",
        "    with torch.no_grad():\n",
        "        logit = pi1.logits[idx].item()\n",
        "    return 1.0 / (1.0 + np.exp(-logit))  # sigmoid\n",
        "\n",
        "\n",
        "def kuhn_prob_action1_p2(pi2, card, history):\n",
        "    idx = p2_infoset_index(card, history)\n",
        "    with torch.no_grad():\n",
        "        logit = pi2.logits[idx].item()\n",
        "    return 1.0 / (1.0 + np.exp(-logit))\n",
        "\n",
        "\n",
        "def kuhn_expected_value(pi1, pi2, pure1=None, pure2=None):\n",
        "    \"\"\"\n",
        "    Exact expected value to player 1 under (pi1, pi2), optionally with pure best\n",
        "    responses overriding one side.\n",
        "\n",
        "    pure1 / pure2: list of length 6 with 0/1 actions per infoset, or None.\n",
        "    \"\"\"\n",
        "\n",
        "    def prob_p1_action1(card, history):\n",
        "        idx = p1_infoset_index(card, history)\n",
        "        if pure1 is not None:\n",
        "            # deterministic: either always 0 or always 1\n",
        "            return float(pure1[idx])\n",
        "        else:\n",
        "            return kuhn_prob_action1_p1(pi1, card, history)\n",
        "\n",
        "    def prob_p2_action1(card, history):\n",
        "        idx = p2_infoset_index(card, history)\n",
        "        if pure2 is not None:\n",
        "            return float(pure2[idx])\n",
        "        else:\n",
        "            return kuhn_prob_action1_p2(pi2, card, history)\n",
        "\n",
        "    total = 0.0\n",
        "    # all 6 valid deals, each with prob 1/6\n",
        "    for card1 in [0,1,2]:\n",
        "        for card2 in [0,1,2]:\n",
        "            if card1 == card2:\n",
        "                continue\n",
        "            p_deal = 1.0 / 6.0\n",
        "\n",
        "            # P1 at root: history \"\"\n",
        "            p1_bet = prob_p1_action1(card1, \"\")\n",
        "            p1_check = 1.0 - p1_bet\n",
        "\n",
        "            # If P1 checks: history \"c\"\n",
        "            p2_bet_after_c = prob_p2_action1(card2, \"c\")\n",
        "            p2_check_after_c = 1.0 - p2_bet_after_c\n",
        "\n",
        "            # If P2 bets after check: history \"cb\"\n",
        "            p1_call_after_cb = prob_p1_action1(card1, \"cb\")\n",
        "            p1_fold_after_cb = 1.0 - p1_call_after_cb\n",
        "\n",
        "            # If P1 bets at root: history \"b\"\n",
        "            p2_call_after_b = prob_p2_action1(card2, \"b\")\n",
        "            p2_fold_after_b = 1.0 - p2_call_after_b\n",
        "\n",
        "            # ----- enumerate all terminal paths -----\n",
        "            # 1) P1 check, P2 check: \"cc\"\n",
        "            prob_cc = p_deal * p1_check * p2_check_after_c\n",
        "            payoff_cc = kuhn_payoff_given_actions(card1, card2, \"cc\")\n",
        "\n",
        "            # 2) P1 check, P2 bet, P1 fold: \"cbf\"\n",
        "            prob_cbf = p_deal * p1_check * p2_bet_after_c * p1_fold_after_cb\n",
        "            payoff_cbf = kuhn_payoff_given_actions(card1, card2, \"cbf\")\n",
        "\n",
        "            # 3) P1 check, P2 bet, P1 call: \"cbc\"\n",
        "            prob_cbc = p_deal * p1_check * p2_bet_after_c * p1_call_after_cb\n",
        "            payoff_cbc = kuhn_payoff_given_actions(card1, card2, \"cbc\")\n",
        "\n",
        "            # 4) P1 bet, P2 fold: \"bf\"\n",
        "            prob_bf = p_deal * p1_bet * p2_fold_after_b\n",
        "            payoff_bf = kuhn_payoff_given_actions(card1, card2, \"bf\")\n",
        "\n",
        "            # 5) P1 bet, P2 call: \"bc\"\n",
        "            prob_bc = p_deal * p1_bet * p2_call_after_b\n",
        "            payoff_bc = kuhn_payoff_given_actions(card1, card2, \"bc\")\n",
        "\n",
        "            total += (\n",
        "                prob_cc * payoff_cc\n",
        "                + prob_cbf * payoff_cbf\n",
        "                + prob_cbc * payoff_cbc\n",
        "                + prob_bf * payoff_bf\n",
        "                + prob_bc * payoff_bc\n",
        "            )\n",
        "\n",
        "    return total\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N8Ao7xLV_m6w"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zZN_sDgr6ula"
      },
      "outputs": [],
      "source": [
        "def best_response_value_to_p2(pi2):\n",
        "    \"\"\"\n",
        "    Returns max_{pure strategies} u1(pure1, pi2)\n",
        "    \"\"\"\n",
        "    best = -999\n",
        "    for pure in PURE_STRATS:\n",
        "        val = kuhn_expected_value(None, pi2, pure1=pure, pure2=None)\n",
        "        if val > best:\n",
        "            best = val\n",
        "    return best\n",
        "\n",
        "\n",
        "def best_response_value_to_p1(pi1):\n",
        "    \"\"\"\n",
        "    Returns min_{pure strategies} u1(pi1, pure2)\n",
        "    (because p2 wants to minimize p1's value)\n",
        "    \"\"\"\n",
        "    worst = 999\n",
        "    for pure in PURE_STRATS:\n",
        "        val = kuhn_expected_value(pi1, None, pure1=None, pure2=pure)\n",
        "        if val < worst:\n",
        "            worst = val\n",
        "    return worst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7qLeRiv53dn"
      },
      "outputs": [],
      "source": [
        "results_ppo = train_kuhn(\n",
        "    mode=\"ppo\",\n",
        "    num_iters=400,\n",
        "    batch_size=512,\n",
        "    lr=3e-4,\n",
        ")\n",
        "\n",
        "results_last = train_kuhn(\n",
        "    mode=\"oppo_last\",\n",
        "    num_iters=400,\n",
        "    batch_size=512,\n",
        "    lr=2e-4,   # slightly smaller for optimistic\n",
        "    omega=0.3,\n",
        ")\n",
        "\n",
        "results_ema = train_kuhn(\n",
        "    mode=\"oppo_ema\",\n",
        "    num_iters=400,\n",
        "    batch_size=512,\n",
        "    lr=1e-4,\n",
        "    omega=0.3,\n",
        "    beta=0.6,\n",
        ")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "iters = range(400)\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(iters, results_ppo[\"p1_returns\"], label=\"PPO p1\")\n",
        "plt.plot(iters, results_last[\"p1_returns\"], label=\"O-PPO-Last p1\")\n",
        "plt.plot(iters, results_ema[\"p1_returns\"], label=\"O-PPO-EMA p1\")\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"avg return (player 1)\")\n",
        "plt.legend()\n",
        "plt.title(\"Kuhn Poker: average return of player 1\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjDf1uyX6xmh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# basic line plot\n",
        "iters = range(len(results_ppo[\"exploitability\"]))\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(iters, results_ppo[\"exploitability\"], label=\"PPO\")\n",
        "plt.plot(iters, results_last[\"exploitability\"], label=\"O-PPO-Last\")\n",
        "plt.plot(iters, results_ema[\"exploitability\"], label=\"O-PPO-EMA\")\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"exploitability\")\n",
        "plt.title(\"Kuhn Poker: exploitability over time\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}